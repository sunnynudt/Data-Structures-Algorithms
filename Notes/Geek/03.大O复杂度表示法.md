### 大 O 复杂度表示法

算法的执行效率，粗略的讲，就是算法代码执行的时间。

学会估算代码执行时间：

```
int cal(int n) {
    int sum = 0;
    int i = 1;
    for (; i <=n ; i++) {
        sum = sum + i;
    }
    return sum;
}
```

从 CPU 的角度来看，这段代码的每一行都执行着类似的操作：**读数据-运算-写数据**。尽管每行代码对应的 CPU 执行的个数、执行的时间都不一样，但是，我们这里只是粗略估计，所以可以假设每行代码执行的时间都一样，为 unit_time。  
第 2、3 行代码分别需要 1 个 `unit_time` 的执行时间，第 4、5 都运行了 n 遍，所以需要`2n * unit_time`的执行时间，所以这段代码总的执行时间就是`(2n + 2) * unit_time`。  
按照这个思路，分析下面代码。

```
int call(int n) {
    int sum = 0;
    int i = 1;
    int j = 1;
    for (; i <= n; ++i) {
        j = 1;
        for (; j <= n; ++j) {
            sum = sum + i * j;
        }
    }
}
```

依旧假设每个语句的执行时间是`unit_time`。
第 2、3、4 行代码，每行需要 1 个`unit_time`的执行时间，第 5、6 行代码循环执行了 n 遍，需要`2n * unit_time`的执行时间，第 7、8 行代码循环执行了`n^2`遍，所以需要`2* n^2 * unit_time`的执行时间。所以，这段代码总的执行时间`T(n) = (2 * n^2 + 2 * n + 3) * unit_time`。

通过上述两段代码执行时间的推导过程，可以得到一个规律：**所有代码的执行时间`T(n)`与每行代码的执行次数 n 成正比**，并可以把这个规律总结成一个公式：**`T(n) = O(f(n))`**。  
其中，`T(n)`表示代码的执行时间；`n`表示数据规模的大小；`f(n)`表示每行代码的次数总和。因为这是一个公式，所以用`f(n)`来表示。公式中的 O，表示代码的执行时间`T(n)`与`f(n)`表达式成正比。

所以，第一个例子中的`T(n) = O(2n + 2)`，第二个例子中的`T(n) = O(2 * n^2 + 2n + 3)`。这就是**大 O 时间复杂度表示法**。大 O 时间复杂度并不表示代码真正的执行时间，而是表示**代码执行时间随  数据规模增大的变化趋势**，所以，也叫作**渐进时间复杂度(asymptotic time complexity)**，简称**时间复杂度**。

当`n`很大的时候，公式中的低阶、常量、系数三部分并左右增长趋势，所以都可以忽略。我们只需要记录一个最大量级就可以了，如果用大 O 表示法  表示刚讲的那两段代码的时间内复杂度，就可以记为：`T(n) = O(n); T(n) = O(n^2)`。
